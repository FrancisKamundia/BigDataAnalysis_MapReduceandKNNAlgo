# BigDataAnalysis_MapReduceandKNNAlgo
Conducting big data analysis using map reduce and K-Means clustering algorithm


ACKNOWLEDGMENTS

  I would like to thank the school’s academic team for the continual support that they gave students in guiding them in the Big Data Analysis Course and other courses as well that yielded the right results needed to accomplish students’ studies.
I would also like to thank my family for their support and patience that they exhibited during the study.

ABSTRACT

  Big Data is a field of study within data science that makes uses of coding methodologies to clean, manipulate, analyze and extract meaning out of data that would usually be considered as humongous in nature and complex. This data ideally would be difficult to comb through using traditional data-processing methodologies and programming environment. Previously, the challenges of data analysis of large data were adding data to the programming environments, reading, capturing, cleaning, running data analysis codes on the data, extracting meaning and presentation of the same.
  Initial research on Big Data was done by Weiss and Indurkhya in the late 1990s. John Mashey is attributed as one of the experts in the field who popularized Big Data. At the time, he was working at Silicon Graphics (Shmueli, G., 2019).
  With the ever-changing world of Information Technology which has led to the change in the threshold of data being large, there has been three stages of Big Data evolution. The first phase is Big Data 1.0 which was pegged on storage and working on data in a Relational Database Management System. This was in the 1990s. In the 2000s, Big Data morphed to semi-structured and unstructured data with the emergence and wide-spread of Internet and Web. This was in construct to what organizations were used to when implementing data analysis from databases. Semi-structured and unstructured data brought about new challenges one of them being how to first convert the data to a structured format before running data analysis methodologies. Big Data Phase 3.0 brought about infinite use cases of data analysis. These include deriving behavioral analysis, Internet of Things, statistical methodologies, quantitative analysis for financial investment firms amongst many others. Along the way, increased wide-spread of internet usage, social media and phone led to availability of millions of data bytes in any given time resulting to development of methodologies to analysis data (Framework, 2019).
Python is the most common used programming language for Big Data because the syntax is simple making it easy to learn. It is open source meaning anyone can access programming environments using python. Python is highly compatible with Hadoop and has many libraries that can be used to analyze data such as pandas and SciPy (Yerukala, M., 2020).

BACKGROUND

  In a MapReduce-based system, input data is broken down into chunks for processing by different worker instances. Each chunk of input data is mapped to an intermediate state using a simple transformation. The intermediate data is then collected together and partitioned based on a key value so that all of the related values are together. Finally, the partitioned data is reduced to a result set. 
  In the past the data collection and diagnosis of patients with mental health disorders has been cumbersome this is because the medical experts still use traditional methods that heavily rely of filling out self-diagnosis reports and the doctor’s application of knowledge, this can often lead to wrong diagnosis of the illness which in turn leads to wrong administration of medicine that can be fatal. In the recent years the field of medicine has seen a rapid increase in application of technology to solve challenges that were slowing down its development towards sustainable solutions.

BODY
  The assignment revolves around finding descriptive statistics for temperature of each day of a given month for the year 2007 while implementing MapReduce Framework to derive daily maximum and minimum, daily mean and median and daily variance. I am also to conduct Cluster Analysis using Apache Mahout while implementing K-Means clustering algorithm.
  To kick off the assignment, I have imported vital python libraries that will aid in reading, writing, implementing, analyzing and presenting the output of the data. The libraries I have imported include pandas, numpy, seabon and matplotlib. 
  The datasets availed for the assignment are 4 text files with a total file size of 590MB. As the file size is within the Big Data file size, I find it better to work with one file. My preference going by my experience is making use of a Comma-Separated Values file. To do this, I have first imported the shutil library. Thereafter, I have written code that prompts the user to input the file name of the four text files and the name of the output file. The input files have first been uploaded into the online coding portal where they will be worked on. The output is saved on the same portal. The code that prompts the user on the details of the files makes the work interactive to the user. The next step is to convert the merged text files to a Comma-Separated Values file where the output file is declared in the code as “bigdatafileall.csv”.
  The first task is tackled by first importing the sys library. The MapReduce Framework is implemented by using different nodes to output the daily minimums and maximums by deriving the Max Mapper, Max Reducer, Min Mapper and Min Reducer. I have used the Hadoop Streaming API to pass data between the Map and Reduce code via STDIN (standard input) and STDOUT (standard output). I will then use Python’s sys.stdin to read input data and print our own output to sys.stdout. The different nodes would be considered as multiprocessors for the assignment. The daily variance is derived by implementing the XXXXXXX library. The Map script will output the temperature values of the stations in tuples immediately. I will let the subsequent Reduce step do the final temperature values. The reducer will read the results of mapper.py from STDIN. Therefore, the output format of mapper.py and the expected input format of reducer.py must match. STDOUT outputs the temperature values of each station. I will then test my mapper.py and reducer.py code before using MapReduce. (M.G.N., 2011).
  To conduct the Cluster Analysis using Apache Mahout, I have implemented the K-Means clustering algorithm. First, I have imported the pandas, numpy, operator and matplotlib.pyplot python libraries. The next step is to load the data set by using the read_csv command and defined the file as the converted csv file “bigdatafileall.csv”. I have then used the head() function to read the titles of the columns of the Comma-Separated Values file. Please note that when converting the merged text files to Comma-Separated Values file, I had added a code to separate the data using commas so that the data can be placed in columns in the output file.
  I will then use random.permutation, development_set, test_set and data.loc commands to divide the dataset into development set and test set and thereafter test the development and test dataset. The next step is to compute the mean and standard deviation of the development and test sets, to compute the normalized euclidean distance. After computing the values, I will then retrieve the class column from the development and test sets and store the output in separate lists. The initial step before running the K-Nearest Neighbors algorithm is to implement Euclidean, Normalized Euclidean and Cosine Similarity methodologies. The output will result to the next step where I will iterate over the development data instance and compute the class for each k value and each distance metric. Computation of the accuracy after the reiterations and optimization of the hyperparameters gets me to a value of 0.9473684210526315 which I have rounded off to 95%

CONCLUSION

  To derive a higher accuracy rate, it is important for a Big Data Analyst to reiterate over the computations and optimization of the hyperparameters. An over ninety percent accuracy rate is ideal for the algorithm. 

REFERENCES

1.	Framework, B. D. (2019, March 26). A short history of Big Data. Enterprise Big Data Framework©. https://www.bigdataframework.org/short-history-of-big-data/
2.	Shmueli, G., Bruce, P. C., Gedeck, P., & Patel, N. R. (2019). Data Mining for Business Analytics: Concepts, Techniques and Applications in Python (1st ed.). Wiley.
3.	Yerukala, M. (2020, November 26). Why is Python a perfect choice for Big Data? Geospatial World. https://www.geospatialworld.net/blogs/why-is-python-a-perfect-choice-for-big-data/
4.	M.G.N. (2011, December 31). Writing An Hadoop MapReduce Program In Python. Michael-Noll. https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/

